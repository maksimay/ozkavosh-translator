CUDA_VISIBLE_DEVICES=0,1 python examples/tacotron2/train_tacotron2.py \
  --train-dir ./dump_ljspeech/train/ \
  --dev-dir ./dump_ljspeech/valid/ \
  --outdir ./examples/tacotron2/exp/train.tacotron2.v2/ \
  --config ./examples/tacotron2/conf/tacotron2.v1.yaml \
  --use-norm 1 \
  --mixed_precision 0 \
  --resume ""

tensorflow-tts-preprocess --rootdir ./ljspeech --outdir ./dump_ljspeech --config preprocess/ljspeech_preprocess.yaml --dataset ljspeech


####### YAML PREPROCESS -> 22050hz wav

metadata.csv erste und letzte zeile löschen ✔


1) debug mels ✔
2) try sampling wider windows --> pitched up audio ✔
3) vocoder is not trained -> mel to wav was trained on english sound (still needs training!)



----> FIX THIS "cause of too many outliers or bad mfa extraction ✔ ✔ ✔ somewhat fixed
IDEAS TO TRY TO FIX MFA PREPROCESS/CYBERBABY PROBLEM:

1) swap out en transcription for oz transcription -> feature extraction on oz text input matches oz audio IF the extraction is able to extract because audio is weird as hell
2) train VOCODER melgan on oz audio -> previously predicted mel has a point of comparison to desired audio output?
3) rewrite ljspeech_mapper somehow?
4) retrain mfa acoustic model somehow? --> works almost =) --> fix MFCC error
0) compare to regular en_training with 10k steps ✔ -> it was indeed broken


12/02/20
DEBUG:
changed en_transcription to oz in metadata.csv and generate_dataset.py
changed n_speakers to 1 from 5 in tensorflowTTS/tensorflowtts/configs/tacotron2.py
changed number of cpus used from 4 to 16 in preprocess.py
changed last phones_mapper entry to be "sil": "END" instead of "sil" ""


13,14/02/20
DEBUG:
GOAL: get mfa to detect something in preprocessing,
0) debug dataset:
  -> goal: create cleaner audio
  split up syllables into speakers? -> problem: not every syll is included in wiki for each demon

16/02/20
KALDI:
1) syllable : phoneme mapping ✔
2) make a pronunciation LEXICON.txt -> oz_word : phonemes ARKOSH AA R K OH SH ✔
3) understand what an utterance is and save start and end points for each utterance in wav ✔

19/02/20 - back to tacotron & kaldi
DEBUG: 'None' bug in phoneme df: there was no mapping in phonem dict! ✔
finish lexicon, finish phoneme mapping ✔
add silence to start and end of utterance ✔
AT SOME POINT: add stresses to phonemes, put similar phonemes on a single line for nonsilent phones list !!!

text.txt ✔ -> words.txt ✔
lexicon ✔ -> filter lexicon ?✔?
segments ✔
wav.scp ✔
utt2spk ✔
spk2utt ✔
nonsilence phones ✔
silence phones ✔

add <oov> to first line of dict ✔

LEXICON RANDOM LINEBREAK ERROR?! -> "fixed" by manually adding: ✔
OOV OOV
<unk> UNK

silence.txt & utt2spk files complaining about whitespace/newlines:
open the file, add a newline, remove the change, re-save the file OKAYDOKAY ✔

global speaker ID fix -> utt_id = spkr_id


"AA",
"AH",
"AO",
"CH",
"D",
"EH",
"F",
"G",
"H",
"IH",
"IY",
"K",
"L",
"M",
"N",
"OH",
"P",
"R",
"S",
"SH",
"T",
"TH",
"UH",
"UW",
"V",
"VO",
"W",
"Y",
"Z",
"ZH",

24/02
4000 lines dataset test!

to do:
create audiolist_csv with glob.glob / remove the need for the condition altogether =)
-> fix index out of range error randompick (file does not exist in folder) ✔
